---
title: "카프카란"
excerpt: "카프카가 무엇인가, 등장배경, Topic, Partition"
toc: true
toc_sticky: true

categories:
  - INFRA/kafka
tags:
  - KAFKA
  - LECTURE
---

*본문은 인프런의 [[데브원영]아파치 카프카 for beginners](https://www.inflearn.com/course/%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4-%EC%9E%85%EB%AC%B8/lecture/67226?tab=note) 강의를 듣고 정리한 내용입니다.*






# Kafka 란 ?

## 카프카의 등장 배경

시간이 지남에 따라 Source Application(데이터 전송 담당)과 Target Application(데이터 수신 담당)이 많아지면서 데이터를 전송하는 라인이 매우 복잡해졌다.

**데이터 전송라인이 많아지면 배포와 장애에 대응하기 어렵다.**

또한 데이터를 전송할 때 프로토콜 포멧의 파편화가 심해졌다. → 추후 데이터의 포멧에 변경사항이 생기면 유지보수가 매우 어려워진다.

이런 복잡함을 해결하기 위해 링크드인에서 개발한 것이 Kafka이다.

## 카프카

카프카는 Source Application과 Target Application의 커플링을 약하게 해준다.

Source Application은 쇼핑몰의 클릭로그, 결제 로그와 같은 데이터를 카프카로 보내고,

Target Application은 카프카로 부터 데이터를 가져와 로그 적재, 로그 처리의 역할을 한다.

Source Application에서 보낼 수 있는 데이터 포멧은 거의 제한이 없다.

![image-20210402184239609](/assets/images/INFRA/kafka/image-20210402184239609.png)

카프카는 각종 데이터를 담는 **Topic(토픽)** 이라는 개념이 있다.

쉽게 **큐** 라고 보면 된다.  Producer는 데이터를 넣고, Consumer는 데이터를 가져간다.

프로듀서와 컨슈머는 라이브러리로 되어있어서, 애플리케이션에서 구현 가능하다.

![image-20210402170855339](/assets/images/INFRA/kafka/image-20210402170855339.png)



카프카는 아주 유연한 큐 역할을 한다.

이러한 데이터 흐름에 있어서 카프카는 fault tolerant 즉, 고가용성으로 서버가 이슈가 생기거나 갑작스럽게 랙(전원)이 내려가는 상황에서도 데이터 손실없이 복구할 수 있다.

또한 낮은 지연(latency)와 높은 처리량(Throughput)을 통해서 아주 효과적으로 데이터를 대량 처리할 수 있다.



# Kafka Topic

토픽 : 데이터가 들어갈 수 있는 공간

토픽은 여러개 생성할 수 있다.

DB의 테이블이나 파일시스템의 폴더와 유사한 성실

이 토픽에 Producer는 데이터를 넣고, Consumer는 데이터를 가져간다.

![image-20210402170855339](/assets/images/INFRA/kafka/image-20210402170855339.png)



토픽에 이름을 지정할 수 있고, 무슨 데이터를 담는지 명확하게 목적에 따라 네이밍을 하면 추후 유지보수 시 편리하게 관리할 수 있다.



## 토픽의 내부

하나의 토픽은 여러개의 파티션으로 구성될 수 있다.

첫 번째 파티션 번호는 0번 부터 시작한다. 

Producer : 내부에 데이터가 파티션 끝에서부터 차곡차곡 쌓이게 된다.

Consumer : 데이터를 가장 오래된 순서대로 가져간다.

<video src="../../../Desktop/topic.mov"></video>

더이상 데이터가 들어오지 않으면 컨슈머는 또 다른 데이터가 들어올 때 까지 기다린다.



* 컨슈머가 데이터를 가져가더라도 데이터는 삭제되지 않는다.

![image-20210402171720659](/assets/images/INFRA/kafka/image-20210402171720659.png)

따라서 새로운 컨슈머가 해당 파티션에 접근하면 0번 부터 가져가서 사용할 수 있다.

**즉, 동일 데이터를 여러번 재사용할 수 있다.** 

이 때 다음 조건을 만족해야 한다.

* 컨슈머 그룹이 달라야한다.
* `auto.offset.reset = earliest` 여야 한다.

![Screen Shot 2021-04-02 at 5.22.18 PM](/assets/images/INFRA/kafka/Screen%20Shot%202021-04-02%20at%205.22.18%20PM.png)

"동일 데이터를 재사용"한다는 것은 카프카를 사용하는 아주 중요한 이유 중 하나이다.

예를 들어,

클릭로그를 토픽에 쌓았을 경우, 

클릭 로그를 ES(일라스틱서치)에 저장해서 데이터를 분석하고 시각화할 수 있고,

클릭로그를 백업하기 위해 Hadoop에 저장할 수도 있다.



파티션이 하나 이상인 경우, 새로운 데이터가 들어가는 방식은?

1. 키가 null이고, 기본 파티셔너를 사용할 경우

   → 라운드 로빈(Round robin)으로 파티션에 돌아가면서 할당된다.

2. 키가 있고, 기본 파티셔너를 사용할 경우 (Producer는 데이터 입력 시 키를 지정할 수 있다.)

   → 키의 해시(hash) 값을 구하고, 특정 파티션에 할당

![image-20210402172606272](/assets/images/INFRA/kafka/image-20210402172606272.png)



## 파티션 늘리기

* **파티션은 늘릴 수 있지만 다시 줄일 수 없기 때문에** 주의해야 한다.



파티션을 늘리는 이유?

* 파티션을 늘리면 컨슈머를 늘려서 데이터 처리를 분산시킬 수 있다.

파티션의 데이터(record)는 언제 삭제되나?

* 삭제되는 타이밍은 옵션에 따라 다르다.
* 레코드가 저장되는 최대 시간과 크기를 지정할 수 있다.
  * `log.retention.ms` : 최대 record 보존 시간
  * `log.retention.byte` : 최대 record 보존 크기(byte)


# 참고하기 좋은 문서

* [Kafka 이해하기](https://medium.com/@umanking/%EC%B9%B4%ED%94%84%EC%B9%B4%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0-%ED%95%98%EA%B8%B0%EC%A0%84%EC%97%90-%EB%A8%BC%EC%A0%80-data%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0%ED%95%B4%EB%B3%B4%EC%9E%90-d2e3ca2f3c2)
* [Apache Kafka 소개 및 아키텍처 정리](https://epicdevs.com/17)

